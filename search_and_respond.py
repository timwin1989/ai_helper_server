import fitz
import json
import faiss
from pathlib import Path
from difflib import SequenceMatcher
from sentence_transformers import SentenceTransformer, CrossEncoder

import requests
from utils import save_html_to_pdf, save_to_docx
from openai import OpenAI
import os
from dotenv import load_dotenv
from topic_utils import infer_topic
load_dotenv()
SUMMARY_CACHE_PATH = "faiss_index/summary_cache.json"
# === –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ===
INDEX_PATH = "faiss_index/index.faiss"
META_PATH = "faiss_index/meta.json"
REQUESTS_DIR = Path("requests")
TOP_K = 4
MODEL_NAME = "mixedbread-ai/mxbai-embed-large-v1" # –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–∏—Å–∫–∞

client = OpenAI()

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ñ–∞–π–ª–∞ –≤ —Å–ª–æ–≤–∞—Ä—å
with open(SUMMARY_CACHE_PATH, "r", encoding="utf-8") as f:
    summary_cache = json.load(f)

# === –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ –∏–Ω–¥–µ–∫—Å–∞ ===
def load_model():
    print(f"üîç –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {MODEL_NAME}")
    return SentenceTransformer(MODEL_NAME)
def load_reranker():
    print("üîç –ó–∞–≥—Ä—É–∑–∫–∞ cross-encoder –¥–ª—è rerank...")
    return CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")

reranker = load_reranker()
model = load_model()
if Path(INDEX_PATH).exists():
    index = faiss.read_index(INDEX_PATH)
else:
    index = faiss.IndexFlatL2(1024)  # –∏–ª–∏ 768, –µ—Å–ª–∏ –¥—Ä—É–≥–∞—è –º–æ–¥–µ–ª—å

if Path(META_PATH).exists():
    with open(META_PATH, encoding="utf-8") as f:
        metadata = json.load(f)
else:
    print("‚ö†Ô∏è META —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞—ë—Ç—Å—è –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫.")
    metadata = []

# === –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ===
def extract_text_from_pdf(path):
    doc = fitz.open(path)
    return "\n".join(page.get_text() for page in doc)

def extract_topic_from_query(query: str) -> str:
    # –ü—Ä–æ—Å—Ç–µ–π—à–∏–π –≤—ã–∑–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–≤–æ–µ–≥–æ indexer'–∞
    keywords = query.lower()
    return infer_topic(keywords)

def filter_similar_chunks(ranked_docs, threshold=0.85):
    filtered = []
    for doc in ranked_docs:
        if all(SequenceMatcher(None, doc["text"], prev["text"]).ratio() < threshold for prev in filtered):
            filtered.append(doc)
    return filtered

def search_by_title_summary(title_query: str, top_k=TOP_K):
    # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ –∫—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ
    combined_texts = [
        f"{m.get('title', '')}. {m.get('summary', '')}".strip()
        for m in metadata
    ]

    # –≠–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞ –∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    doc_vectors = model.encode(combined_texts)
    query_vector = model.encode([title_query])

    # –í—Ä–µ–º–µ–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å –∏ –ø–æ–∏—Å–∫
    temp_index = faiss.IndexFlatL2(doc_vectors.shape[1])
    temp_index.add(doc_vectors)
    distances, indices = temp_index.search(query_vector, top_k)

    # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    retrieved = [metadata[i] for i in indices[0] if i < len(metadata)]
    return retrieved


def search(query, title: str = "", top_k=TOP_K):
    topic = extract_topic_from_query(query)
    topic_matched = [m for m in metadata if m.get("topic") == topic] or metadata  # fallback –Ω–∞ –≤—Å—ë

    # # –ü–æ–ø—ã—Ç–∫–∞ 1: –∏—â–µ–º —Ç–æ–ª—å–∫–æ –ø–æ title
    # if title:
    #     combined_texts = [m.get("title", "") for m in topic_matched]
    #     vectors = model.encode(combined_texts)
    #     query_vec = model.encode([title])
    #     temp_index = faiss.IndexFlatL2(vectors.shape[1])
    #     temp_index.add(vectors)
    #     distances, indices = temp_index.search(query_vec, top_k)

    #     retrieved = [topic_matched[i] for i in indices[0] if i < len(topic_matched)]
    #     reranked = rerank_results(query, retrieved)
    #     deduped = filter_similar_chunks(reranked)

    #     # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É ‚Äî fallback –Ω–∞ title + text
    #     if deduped:
    #         return deduped[:top_k]

    # –ü–æ–ø—ã—Ç–∫–∞ 2: –ø–æ title + text
    
    combined_texts = [f"{m.get('title', '')} {m.get('text', '')}".strip() for m in topic_matched]
    query_vec = model.encode([query])
    doc_vecs = model.encode(combined_texts)

    temp_index = faiss.IndexFlatL2(doc_vecs.shape[1])
    temp_index.add(doc_vecs)
    distances, indices = temp_index.search(query_vec, top_k * 2)

    retrieved = []
    for i in indices[0]:
        if i < len(topic_matched):
            doc = topic_matched[i]
            doc["__combined_text__"] = combined_texts[i]
            retrieved.append(doc)

    reranked = rerank_results(query, retrieved)
    deduped = filter_similar_chunks(reranked)

    return deduped[:top_k]

def rerank_results(query, retrieved):
    if not retrieved:
        return []

    pairs = [(query, doc["text"]) for doc in retrieved]
    scores = reranker.predict(pairs)
    scored_docs = list(zip(retrieved, scores))
    ranked = sorted(scored_docs, key=lambda x: x[1], reverse=True)
    return [doc for doc, _ in ranked]

def query_ollama(prompt, model_ollama="mistral:instruct"):
    try:
        response = requests.post(
            "http://localhost:11434/api/generate",
            json={"model": model_ollama, "prompt": prompt, "stream": False}
        )
        data = response.json()
        return data.get("response", "[–û—à–∏–±–∫–∞ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏]")
    except Exception as e:
        return f"[–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama: {e}]"

def query_openai(system_prompt: str, prompt: str, temperature: int = 1, model_open_ai: str = "gpt-4o") -> str:
    try:
        response = client.chat.completions.create(
            model=model_open_ai,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            temperature=temperature,
        )
        content = response.choices[0].message.content.strip()

        # üîç –£–¥–∞–ª—è–µ–º –æ–±—ë—Ä—Ç–∫–∏ ```json –∏ ```
        if content.startswith("```json") or content.startswith("```"):
            content = content.replace("```json", "").replace("```", "").strip()

        # ‚úÇÔ∏è –£–¥–∞–ª—è–µ–º –ø–æ—è—Å–Ω–µ–Ω–∏–µ –≤ –Ω–∞—á–∞–ª–µ, –µ—Å–ª–∏ –æ–Ω–æ –µ—Å—Ç—å
        for prefix in [
            "html",
        ]:
            if content.startswith(prefix):
                content = content[len(prefix):].strip()
                break  # —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—É—é –ø–æ–¥—Ö–æ–¥—è—â—É—é —Å—Ç—Ä–æ–∫—É —É–¥–∞–ª—è–µ–º

        return content

    except Exception as e:
        return f"[–û—à–∏–±–∫–∞ OpenAI: {e}]"

def generate_answer(request_text, similar_docs, system_prompt: str, lang: str = "–†—É—Å—Å–∫–∏–π", use_openai=False):


    shortened_docs = []
    fragments_list = []
    for i, doc in enumerate(similar_docs):
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ text, —á—Ç–æ–±—ã –æ–Ω —Ç–æ—á–Ω–æ –ø—Ä–æ—à—ë–ª —Ñ–∏–ª—å—Ç—Ä
        base_text = doc.get('text', '').strip().replace("\n", " ")
        if len(base_text) < 20:
            continue
        if len(base_text) > 2000:
            base_text = base_text[:2000] + "..."

        # –í—ã–≤–æ–¥–∏–º –≤—Å–µ–≥–¥–∞ context_text, –¥–∞–∂–µ –µ—Å–ª–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —à–ª–∞ –ø–æ text
        context = doc.get('context_text', '').strip().replace("\n", " ")
        shortened_docs.append(f"[–§—Ä–∞–≥–º–µ–Ω—Ç {i+1} ‚Äî –∏—Å—Ç–æ—á–Ω–∏–∫: {doc['source']}]\n{context}")

        source = doc.get("source")
        summary_entry = summary_cache.get(source)

        title = summary_entry.get("title") if isinstance(summary_entry, dict) else None

        fragments_list.append({
            "title": title,
            "file_name": source,
            "context": context
        })

    if not shortened_docs:
        shortened_docs.append("[–ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.]")
 

    similar_text_block = "\n\n".join(shortened_docs)


    prompt = f"""
        –í–æ—Ç —Ç–µ–∫—Å—Ç –¥–µ–ø—É—Ç–∞—Ç—Å–∫–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞:
        {request_text}

        –í–æ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∏–∑ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤:
        {similar_text_block}
    """

    print("\nüì§ –ü—Ä–æ–º–ø—Ç, –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –≤ LLM:\n")
    print(prompt)

    if use_openai:
        return {
            "ai_answer": query_openai(system_prompt, prompt+f"""–û—Ç–≤–µ—Ç –ø–µ—Ä–µ–≤–µ–¥–∏ –Ω–∞ —è–∑—ã–∫: {lang}"""),
            "fragments_list": fragments_list
        }
    else:
        return {
            "ai_answer": query_ollama(system_prompt, prompt+f"""–û—Ç–≤–µ—Ç –ø–µ—Ä–µ–≤–µ–¥–∏ –Ω–∞ —è–∑—ã–∫: {lang}"""),
            "fragments_list": fragments_list
        }

# === –û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—É—Å–∫ ===
if __name__ == "__main__":
    request_files = list(REQUESTS_DIR.glob("*.pdf"))
    if not request_files:
        print("‚ùå –í –ø–∞–ø–∫–µ 'requests/' –Ω–µ—Ç PDF-–∑–∞–ø—Ä–æ—Å–æ–≤.")
        exit(1)

    request_path = request_files[0]
    request_text = extract_text_from_pdf(str(request_path))
    print(f"\nüìÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –∑–∞–ø—Ä–æ—Å –∏–∑ —Ñ–∞–π–ª–∞: {request_path.name}")

    retrieved_docs = search(request_text, "–ó–∞–≥–æ–ª–æ–≤–æ–∫")
    similar_docs = [{"text": r["text"], "source": r.get("source", "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–∞–π–ª")} for r in retrieved_docs]

    if not similar_docs:
        print("‚ùó –ù–∏—á–µ–≥–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
        similar_docs = [{"text": "(–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö)", "source": "N/A"}]

    final = generate_answer(request_text, similar_docs, "–†—É—Å—Å–∫–∏–π", use_openai=True)

    print("\n‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç:\n")
    print(final)

    save_html_to_pdf(final, "generateAnswer.pdf")
