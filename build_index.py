import fitz  # PyMuPDF
import faiss
import json
import os
import re
import docx
from pathlib import Path
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
from hashlib import md5
from search_and_respond import query_openai
from kazakh_translator import translate_kazakh_to_russian
from topic_utils import infer_topic

SUMMARY_CACHE_PATH = "faiss_index/summary_cache.json"
summary_cache = {}
if Path(SUMMARY_CACHE_PATH).exists():
    with open(SUMMARY_CACHE_PATH, "r", encoding="utf-8") as f:
        summary_cache = json.load(f)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
CHUNK_SIZE = 1000
INDEX_PATH = "faiss_index/index.faiss"
META_PATH = "faiss_index/meta.json"
META_JSONL = "faiss_index/meta.jsonl"

os.makedirs("faiss_index", exist_ok=True)

model = SentenceTransformer("mixedbread-ai/mxbai-embed-large-v1")

if Path(INDEX_PATH).exists():
    index = faiss.read_index(INDEX_PATH)
else:
    index = faiss.IndexFlatL2(1024)

if Path(META_PATH).exists():
    with open(META_PATH, encoding="utf-8") as f:
        metadata = json.load(f)
else:
    metadata = []

already_indexed = set((m["source"], md5(m["text"].encode()).hexdigest()) for m in metadata)


def force_translate_to_russian(text: str) -> str:
    text = text.strip()
    if not text or len(text) < 30:
        return text

    try:
        translated = translate_kazakh_to_russian(text)

        return translated
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–≤–æ–¥–µ: {e}")
        return text

def extract_text_from_pdf(path):
    try:
        doc = fitz.open(path)
        return "\n".join([page.get_text() for page in doc])
    except Exception as e:
        print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å PDF {path}: {e}")
        return ""

def extract_text_from_file(file: Path) -> str:
    if file.suffix.lower() == ".pdf":
        return extract_text_from_pdf(file)
    elif file.suffix.lower() == ".docx":
        return extract_text_from_docx(file)
    else:
        print(f"‚ö†Ô∏è –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç: {file.name}")
        return ""

def chunk_text_with_overlap(text: str, size=1000, overlap=200):
    words = text.split()
    chunks = []
    start = 0
    while start < len(words):
        end = start + size
        chunk = " ".join(words[start:end])
        chunks.append(chunk.strip())
        start += size - overlap
    return chunks

def infer_date(text):
    match = re.search(r"\d{1,2}\s+[–∞-—è–ê-–Ø]+\s+20\d{2}", text)
    return match.group(0) if match else None

def extract_text_from_docx(path):
    try:
        doc = docx.Document(path)
        return "\n".join([para.text for para in doc.paragraphs])
    except Exception as e:
        print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å {path}: {e}")
        return ""

def summarize_text(text: str, filename: str) -> dict:
    if filename in summary_cache:
        return summary_cache[filename]

    try:
        system_prompt = (
            "–¢—ã ‚Äî –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ –¥–µ–ø—É—Ç–∞—Ç–∞. –ü—Ä–æ—á–∏—Ç–∞–π —Ç–µ–∫—Å—Ç –∏ –≤–µ—Ä–Ω–∏ JSON-–æ–±—ä–µ–∫—Ç —Å –¥–≤—É–º—è –∫–ª—é—á–∞–º–∏:\n"
            "1. `title` ‚Äî –∫–æ—Ä–æ—Ç–∫–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ (–¥–æ 100 —Å–∏–º–≤–æ–ª–æ–≤), –æ—Ç—Ä–∞–∂–∞—é—â–∏–π —Å—É—Ç—å –æ–±—Ä–∞—â–µ–Ω–∏—è;\n"
            "2. `summary` ‚Äî –∫—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ (–¥–æ 1700 —Å–∏–º–≤–æ–ª–æ–≤) —Å–∞–º–æ–≥–æ –≤–∞–∂–Ω–æ–≥–æ.\n"
            "–û—Ç–≤–µ—Ç —Å—Ç—Ä–æ–≥–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON, –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π."
        )
        response = query_openai(system_prompt, text)

        try:
            parsed = json.loads(response)
            summary_cache[filename] = parsed
            with open(SUMMARY_CACHE_PATH, "w", encoding="utf-8") as f:
                json.dump(summary_cache, f, ensure_ascii=False, indent=2)
            return parsed
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Ä–∞–∑–±–æ—Ä–∞ JSON: {e}\n–û—Ç–≤–µ—Ç: {response}")
            return {"title": filename, "summary": text[:400] + "..."}

    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã–∂–∏–º–∫–∏: {e}")
        return {"title": filename, "summary": text[:400] + "..."}

all_files = [f for f in Path("pdfs").iterdir() if f.suffix.lower() in [".pdf", ".docx"] and not f.name.startswith("~")]
for file in tqdm(all_files, desc="üìÑ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"):
    try:
        full_text = extract_text_from_file(file)
        if not full_text.strip():
            print(f"‚ö†Ô∏è –ü—É—Å—Ç–æ–π –¥–æ–∫—É–º–µ–Ω—Ç: {file.name}")
            continue

        translated_full_text = force_translate_to_russian(full_text) 
        summary = summarize_text(translated_full_text, file.name)
        print("üìå –ó–∞–≥–æ–ª–æ–≤–æ–∫:", summary["title"])

        chunks = chunk_text_with_overlap(full_text, size=CHUNK_SIZE, overlap=200)

        new_chunks = []
        for chunk in chunks:
            translated = force_translate_to_russian(chunk)
            chunk_hash = md5(translated.encode()).hexdigest()
            if (file.name, chunk_hash) not in already_indexed:
                new_chunks.append((translated, chunk_hash))

        if not new_chunks:
            continue

        embeddings = model.encode([chunk for chunk, _ in new_chunks])
        index.add(embeddings)

        for chunk, chunk_hash in new_chunks:
            item = {
                "source": file.name,
                "title": summary["title"],
                "document": file.stem,
                "text": chunk,
                "hash": chunk_hash,
                "context_text": summary["summary"],
                "chunk_length": len(chunk),
                "preview": chunk[:80].replace("\n", " ") + "...",
                "date": infer_date(chunk),
                "topic": infer_topic(chunk),
                "lang": "ru"
            }
            metadata.append(item)

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {file.name}: {e}")

faiss.write_index(index, INDEX_PATH)

with open(META_PATH, "w", encoding="utf-8") as f:
    json.dump(metadata, f, ensure_ascii=False, indent=2)

with open(META_JSONL, "w", encoding="utf-8") as f:
    for entry in metadata:
        f.write(json.dumps(entry, ensure_ascii=False) + "\n")

print("‚úÖ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")
